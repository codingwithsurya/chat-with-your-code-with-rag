{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with your code! </>\n",
    "\n",
    "\n",
    "<img src=\"chat_with_code.png\" width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/teamspace/studios/this_studio/weights\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/teamspace/studios/this_studio/weights\"\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import uuid\n",
    "import textwrap\n",
    "import subprocess\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "from rag_101.retriever import (\n",
    "    load_embedding_model,\n",
    "    load_reranker_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows nested access to the event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the llm\n",
    "llm=Ollama(model=\"llama3\", request_timeout=60.0)\n",
    "\n",
    "# setting up the embedding model\n",
    "lc_embedding_model = load_embedding_model()\n",
    "embed_model = LangchainEmbedding(lc_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def parse_github_url(url):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "def clone_github_repo(repo_url):    \n",
    "    try:\n",
    "        print('Cloning the repo ...')\n",
    "        result = subprocess.run([\"git\", \"clone\", repo_url], check=True, text=True, capture_output=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to clone repository: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a query engine\n",
    "\n",
    "def setup_query_engine(github_url):\n",
    "    \n",
    "    owner, repo = parse_github_url(github_url)\n",
    "    \n",
    "    if validate_owner_repo(owner, repo):\n",
    "        # Clone the GitHub repo & save it in a directory\n",
    "        input_dir_path = f\"/teamspace/studios/this_studio/{repo}\"\n",
    "\n",
    "        if os.path.exists(input_dir_path):\n",
    "            pass\n",
    "        else:\n",
    "            clone_github_repo(github_url)\n",
    "        \n",
    "        loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".py\", \".ipynb\", \".js\", \".ts\", \".md\"],\n",
    "            recursive=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            docs = loader.load_data()\n",
    "\n",
    "            # ====== Create vector store and upload data ======\n",
    "            Settings.embed_model = embed_model\n",
    "            index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
    "            # TODO try async index creation for faster emebdding generation & persist it to memory!\n",
    "            # index = VectorStoreIndex(docs, use_async=True)\n",
    "\n",
    "            # ====== Setup a query engine ======\n",
    "            Settings.llm = llm\n",
    "            query_engine = index.as_query_engine(similarity_top_k=4)\n",
    "            \n",
    "            # ====== Customise prompt template ======\n",
    "            qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "            qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "            query_engine.update_prompts(\n",
    "                {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    "            )\n",
    "\n",
    "            if docs:\n",
    "                print(\"Data loaded successfully!!\")\n",
    "                print(\"Ready to chat!!\")\n",
    "            else:\n",
    "                print(\"No data found, check if the repository is not empty!\")\n",
    "            \n",
    "            return query_engine\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        print('Invalid github repo, try again!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03467781f90d491da4bac23aaa27b2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0202a691d94181a1cea3bb7341e3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!!\n",
      "Ready to chat!!\n"
     ]
    }
   ],
   "source": [
    "# Provide url to the repository you want to chat with\n",
    "github_url = \"https://github.com/Lightning-AI/lit-gpt\"\n",
    "\n",
    "query_engine = setup_query_engine(github_url=github_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the context information provided, I'll do my best to give a detailed overview of the repository.\n",
       "\n",
       "**Repository Structure**\n",
       "The repository appears to be related to LitGPT (Lightweight Generalized Pre-training), which is an open-source project for building AI models. The file structure suggests that it contains various files and directories related to tutorials, documentation, and code.\n",
       "\n",
       "The top-level directory `/teamspace/studios/this_studio/lit-gpt` appears to be the root directory of the repository. Within this directory, there are several subdirectories and files:\n",
       "\n",
       "* `tutorials`: This directory contains tutorial-related files, including Markdown files (`README.md`, `0_to_litgpt.md`) and possibly other documentation.\n",
       "* `extensions`: This directory might contain code for LitGPT extensions or plugins.\n",
       "* `lit-gpt`: This directory likely holds the main LitGPT project code and related files.\n",
       "\n",
       "**File Descriptions**\n",
       "Some notable files mentioned in the context information are:\n",
       "\n",
       "* `README.md`: A Markdown file providing general information about the repository, its purpose, and how to contribute.\n",
       "* `tutorials/0_to_litgpt.md`: A tutorial or guide on getting started with LitGPT.\n",
       "* `resource-tables.md`: A table-based document possibly listing resources or benchmarks related to LitGPT.\n",
       "\n",
       "**ThunderFSDPStrategy**\n",
       "The context information also mentions a custom strategy called `ThunderFSDPStrategy`. This appears to be a specific implementation for parallel processing and sharding in the LitGPT project. The details of this strategy are not provided, but it seems to involve sharding strategies (e.g., \"ZERO3\"), bucketing strategies (\"BLOCK\"), and multiple executors.\n",
       "\n",
       "**Contributor Information**\n",
       "The repository's README file encourages contributors by stating that all individual contributors, regardless of their level of experience or hardware, are welcome. The file also provides links for feature requests, questions, or code contributions through the GitHub Issue tracker.\n",
       "\n",
       "**Open Questions**\n",
       "While I've tried to provide a detailed overview based on the provided context information, there might be additional aspects or details about this repository that are not immediately clear. If you'd like me to explore specific topics further or have follow-up questions, please let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query('Tell me about this repisitory! in detail. like the file structure')\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
