{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/teamspace/studios/this_studio/weights\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/teamspace/studios/this_studio/weights\"\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import uuid\n",
    "import textwrap\n",
    "import subprocess\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "from rag_101.retriever import (\n",
    "    load_embedding_model,\n",
    "    load_reranker_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows nested access to the event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the llm\n",
    "llm=Ollama(model=\"llama3\", request_timeout=60.0)\n",
    "\n",
    "# setting up the embedding model\n",
    "lc_embedding_model = load_embedding_model()\n",
    "embed_model = LangchainEmbedding(lc_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def parse_github_url(url):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "def clone_github_repo(repo_url):    \n",
    "    try:\n",
    "        print('Cloning the repo ...')\n",
    "        result = subprocess.run([\"git\", \"clone\", repo_url], check=True, text=True, capture_output=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to clone repository: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a query engine\n",
    "\n",
    "def setup_query_engine(github_url):\n",
    "    \n",
    "    owner, repo = parse_github_url(github_url)\n",
    "    \n",
    "    if validate_owner_repo(owner, repo):\n",
    "        # Clone the GitHub repo & save it in a directory\n",
    "        input_dir_path = f\"/teamspace/studios/this_studio/{repo}\"\n",
    "\n",
    "        if os.path.exists(input_dir_path):\n",
    "            pass\n",
    "        else:\n",
    "            clone_github_repo(github_url)\n",
    "        \n",
    "        loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".py\", \".ipynb\", \".js\", \".ts\", \".md\"],\n",
    "            recursive=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            docs = loader.load_data()\n",
    "\n",
    "            # ====== Create vector store and upload data ======\n",
    "            Settings.embed_model = embed_model\n",
    "            index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
    "            # ====== Setup a query engine ======\n",
    "            Settings.llm = llm\n",
    "            query_engine = index.as_query_engine(similarity_top_k=4)\n",
    "            \n",
    "            # ====== Customise prompt template ======\n",
    "            qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"You are llama3, a large language model developed by Meta AI. Surya has integrated you into this environment so you can answer any user's coding questions! Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "            qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "            query_engine.update_prompts(\n",
    "                {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    "            )\n",
    "\n",
    "            if docs:\n",
    "                print(\"Data loaded successfully!!\")\n",
    "                print(\"Ready to chat!!\")\n",
    "            else:\n",
    "                print(\"No data found, check if the repository is not empty!\")\n",
    "            \n",
    "            return query_engine\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        print('Invalid github repo, try again!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning the repo ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325eba87001a4ddb918ae753ba42d4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcab5cdae0740d8bbc937d318a4860b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!!\n",
      "Ready to chat!!\n"
     ]
    }
   ],
   "source": [
    "# Provide url to the repository you want to chat with\n",
    "github_url = \"https://github.com/meta-llama/llama3\"\n",
    "\n",
    "query_engine = setup_query_engine(github_url=github_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm LLaMA3!\n",
       "\n",
       "Given the context information provided, I can help you understand how KV caching is used in `model.py` along with the attention mechanism.\n",
       "\n",
       "KV caching is a technique used to improve the efficiency of transformer-based models like ours. In our case, we use KV caching to store the attention scores for tokens that have been processed previously.\n",
       "\n",
       "Here's how it works:\n",
       "\n",
       "1. **Attention Scores Calculation**: When processing a token, we calculate its attention scores using the attention mechanism (a combination of self-attention and cross-attention). These attention scores represent the importance of each token in the input sequence with respect to the current token.\n",
       "2. **KV Cache Construction**: We create a cache that stores these attention scores for tokens that have been processed previously. This cache is divided into two parts: Key (K) and Value (V).\n",
       "3. **KV Caching**: When processing a new token, we first check if its attention scores are already cached in the KV cache. If they are, we can reuse those scores instead of recalculating them from scratch.\n",
       "4. **Attention Mechanism with KV Cache**: In the attention mechanism, we use the KV cache to retrieve the attention scores for tokens that have been processed previously. We then use these cached scores to compute the attention weights.\n",
       "\n",
       "Here's a snippet from `model.py` that illustrates how KV caching is used in conjunction with the attention mechanism:\n",
       "```python\n",
       "def forward(self, x: torch.Tensor, start_pos: int):\n",
       "    ...\n",
       "    for layer in self.layers:\n",
       "        h = layer(h, start_pos, freqs_cis, mask)\n",
       "    ...\n",
       "```\n",
       "In this code snippet, `layer` represents a TransformerBlock, which applies the attention mechanism to the input sequence. The `start_pos` variable indicates the starting position of the token being processed.\n",
       "\n",
       "The `freqs_cis` tensor is used as an input to the attention mechanism, and it's where KV caching comes into play. We cache the attention scores for tokens that have been processed previously in a KV cache (Key-Value cache). When processing a new token, we check if its attention scores are already cached in the KV cache. If they are, we reuse those cached scores instead of recalculating them from scratch.\n",
       "\n",
       "This caching mechanism helps improve the efficiency of our model by reducing the computational complexity of attention score calculation and allowing us to leverage previously computed attention scores when processing subsequent tokens.\n",
       "\n",
       "I hope this explanation helps you understand how KV caching is used in conjunction with the attention mechanism in `model.py`!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query('Tell me in detail how we use kv cache in model.py with attention mechanism?')\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
