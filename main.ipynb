{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/teamspace/studios/this_studio/weights\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/teamspace/studios/this_studio/weights\"\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import uuid\n",
    "import textwrap\n",
    "import subprocess\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "from rag_101.retriever import (\n",
    "    load_embedding_model,\n",
    "    load_reranker_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows nested access to the event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the llm\n",
    "llm=Ollama(model=\"llama3\", request_timeout=60.0)\n",
    "\n",
    "# setting up the embedding model\n",
    "lc_embedding_model = load_embedding_model()\n",
    "embed_model = LangchainEmbedding(lc_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def parse_github_url(url):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "def clone_github_repo(repo_url):    \n",
    "    try:\n",
    "        print('Cloning the repo ...')\n",
    "        result = subprocess.run([\"git\", \"clone\", repo_url], check=True, text=True, capture_output=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to clone repository: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a query engine\n",
    "\n",
    "def setup_query_engine(github_url):\n",
    "    \n",
    "    owner, repo = parse_github_url(github_url)\n",
    "    \n",
    "    if validate_owner_repo(owner, repo):\n",
    "        # Clone the GitHub repo & save it in a directory\n",
    "        input_dir_path = f\"/teamspace/studios/this_studio/{repo}\"\n",
    "\n",
    "        if os.path.exists(input_dir_path):\n",
    "            pass\n",
    "        else:\n",
    "            clone_github_repo(github_url)\n",
    "        \n",
    "        loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".py\", \".ipynb\", \".js\", \".ts\", \".md\"],\n",
    "            recursive=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            docs = loader.load_data()\n",
    "\n",
    "            # ====== Create vector store and upload data ======\n",
    "            Settings.embed_model = embed_model\n",
    "            index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
    "            # ====== Setup a query engine ======\n",
    "            Settings.llm = llm\n",
    "            query_engine = index.as_query_engine(similarity_top_k=4)\n",
    "            \n",
    "            # ====== Customise prompt template ======\n",
    "            qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"You are llama3, a large language model developed by Meta AI. Surya has integrated you into this environment so you can answer any user's coding questions! Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "            qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "            query_engine.update_prompts(\n",
    "                {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    "            )\n",
    "\n",
    "            if docs:\n",
    "                print(\"Data loaded successfully!!\")\n",
    "                print(\"Ready to chat!!\")\n",
    "            else:\n",
    "                print(\"No data found, check if the repository is not empty!\")\n",
    "            \n",
    "            return query_engine\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        print('Invalid github repo, try again!')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning the repo ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595e1bef4b8040388f517c2fd4799562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a7820dc0e44f27ae2921f0be16a56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!!\n",
      "Ready to chat!!\n"
     ]
    }
   ],
   "source": [
    "# Provide url to the repository you want to chat with\n",
    "github_url = \"https://github.com/Lightning-AI/lit-gpt\"\n",
    "\n",
    "query_engine = setup_query_engine(github_url=github_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello there! I'm llama3, and I'm here to help you with your query.\n",
       "\n",
       "To finetune a model using LitGPT, you can follow these steps:\n",
       "\n",
       "**Step 1: Choose the Finetuning Method**\n",
       "LitGPT provides several methods for supervised instruction finetuning. The supported methods are:\n",
       "```bash\n",
       "litgpt finetune full\n",
       "litgpt finetune lora\n",
       "litgpt finetune adapter\n",
       "litgpt finetune adapter_v2\n",
       "```\n",
       "In this example, I'll use the LoRA (Low-Rank Adaptation) method.\n",
       "\n",
       "**Step 2: Prepare Your Dataset**\n",
       "Your dataset should be formatted in a specific way. LitGPT supports datasets with an `input` field and those without. For instruction-finestuning, you can use the following format:\n",
       "```\n",
       "{\n",
       "    \"input\": \"<instruction_text>\",\n",
       "    \"target\": \"<target_text>\"\n",
       "}\n",
       "```\n",
       "Alternatively, your dataset might contain an `input` field.\n",
       "\n",
       "**Step 3: Download a Pretrained Model**\n",
       "To start finetuning, you need to download a pretrained model. For example:\n",
       "```bash\n",
       "litgpt download --repo_id microsoft/phi-2\n",
       "```\n",
       "This will download the `microsoft/phi-2` model.\n",
       "\n",
       "**Step 4: Configure Finetuning with LitGPT**\n",
       "You can use command-line arguments and configuration files to finetune your model. Here's an example using LoRA:\n",
       "```bash\n",
       "litgpt finetune lora \\\n",
       "    --config config_hub/finetune/phi-2/lora.yaml \\\n",
       "    --train.max_steps 5\n",
       "```\n",
       "This will finetune the downloaded `microsoft/phi-2` model using LoRA and train for a maximum of 5 steps.\n",
       "\n",
       "**Step 5: Run Finetuning**\n",
       "Once you've configured LitGPT, you can run the finetuning process:\n",
       "```bash\n",
       "litgpt finetune lora ...\n",
       "```\n",
       "This will start training your model on the prepared dataset.\n",
       "\n",
       "That's it! You should now have a good understanding of how to fine-tune using LitGPT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query('Tell me in detail about how to fine tune using lit-gpt?')\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
